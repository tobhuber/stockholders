\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\begin{document}

Three fundamental elements:
\begin{itemize}
\item statistical model describing a method’s general functional form for risk premium
predictions
\item second is an objective function for estimating model parameters
\item even with a small number of predictors, the set of model permutations expands rapidly when one considers nonlinear predictor
transformations.
\end{itemize}

\section{Methodology}

\subsection{Sample Splitting And Tuning Via Validation}
	Hyperparameters (initial parameters set prior to the machine learning process)
	drastically affect the complexity and therefore overall performance of the model.
	An adaptive approach is used where the sample is split into the 3 disjoint time periods
	"training", "validation" and "testing". The "training" period estimates the model
	subject to a specific set of tuning parameter values. The "validation" period
	tunes the parameters by simulating an out-of-sample-test of the model
	to produce reliable out-of-sample performance. The "testing" period is truly
	out-of-sample and therefore is used to evaluate a method’s predictiveperformance.

\subsection{Simple Linear}
	Describing the model by simple linear predictive regression it will perform poorly in a high 
	dimension problem nevertheless we use it as reference point for emphasizing the distinctive 
	features of more sophisticated methods.
	However it does not allow for nonlinear effects or interactions between predictors.
	Yet in some cases it is possible to improve accuracy by using weighted least squares.
	Thus allowing statistically or economically information.
	
\subsection{Penalized Linear}
	When the number of predictors P approaches the number of observations T,
	the linear model begins to overfit noise and thus becomes inefficient or inconsistent.
	This problem obviously gets even worse with a bad signal-to-noise ratio.
	To avoid overfit the number of estimated paramters must be reduced.
	The idea to achieve this is to penalize the objective function with a penalty
	function (e.g. the elastic net) and thus deteriorates the in-sample performance
	in hopes of improving the out-of-sample stability.

\subsection{Dimension Reduction: PCR and PLS}
\subsection{Generalized Linear}
	Finding a linear model for a complex and nonlinear function introduces some problems:
	\begin{itemize}
		\item approximation errors
		\item estimation errors
		\item intrinsic errors
	\end{itemize}
	Approximation error can be reduced by including more flexible specifications to improve the 
	models ability to approximate the 'true' model.
	Estimation error is determined by the date.
	Intrinsic error cannot be reduced as it is the general randomness in financial markets.
	The Generalized Linear Model uses nonlinear transformations of the original predictors as new
	additive terms in an otherwise linear model.
	Higher order terms enter additively thus predicting the outcome can be achieved with the same
	tools used for the Simple Linear Model.
	Using penalization is necessary due to expansion quickly multiplies the number of model parameter.
	For this the 'group lasso' a specialized spline function is used.
	
\subsection{Boosted Regression Trees and Random Forest}
\subsection{Neural Networks}


\end{document}
